{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Turing.jl Turing is a universal probabilistic programming language with a focus on intuitive modelling interface, composable probabilistic inference and computational scalability. Features Turing provides Hamiltonian Monte Carlo (HMC) and particle MCMC sampling algorithms for complex posterior distributions (e.g. those involving discrete variables and stochastic control flows). Current features include: Universal probabilistic programming with an intuitive modelling interface Hamiltonian Monte Carlo (HMC) sampling for differentiable posterior distributions Particle MCMC sampling for complex posterior distributions involving discrete variables and stochastic control flows Gibbs sampling that combines particle MCMC, HMC and many other MCMC algorithms Resources Please visit Turing.jl wiki for documentation, tutorials (e.g. get started ) and other topics (e.g. advanced usages ). Below are some example models for Turing. Introduction Gaussian Mixture Model Bayesian Hidden Markov Model Factorical Hidden Markov Model Topic Models: LDA and MoC Citing Turing To cite Turing, please refer to the following paper. Sample BibTeX entry is given below: @InProceedings{turing17, title = {{T}uring: a language for flexible probabilistic inference}, author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin}, booktitle = {Proceedings of the 21th International Conference on Artificial Intelligence and Statistics}, year = {2018}, series = {Proceedings of Machine Learning Research}, publisher = {PMLR}, } Other probablistic/deep learning languages Stan Infer.NET PyTorch / Pyro TensorFlow / Edward DyNet","title":"Home"},{"location":"#turingjl","text":"Turing is a universal probabilistic programming language with a focus on intuitive modelling interface, composable probabilistic inference and computational scalability.","title":"Turing.jl"},{"location":"#features","text":"Turing provides Hamiltonian Monte Carlo (HMC) and particle MCMC sampling algorithms for complex posterior distributions (e.g. those involving discrete variables and stochastic control flows). Current features include: Universal probabilistic programming with an intuitive modelling interface Hamiltonian Monte Carlo (HMC) sampling for differentiable posterior distributions Particle MCMC sampling for complex posterior distributions involving discrete variables and stochastic control flows Gibbs sampling that combines particle MCMC, HMC and many other MCMC algorithms","title":"Features"},{"location":"#resources","text":"Please visit Turing.jl wiki for documentation, tutorials (e.g. get started ) and other topics (e.g. advanced usages ). Below are some example models for Turing. Introduction Gaussian Mixture Model Bayesian Hidden Markov Model Factorical Hidden Markov Model Topic Models: LDA and MoC","title":"Resources"},{"location":"#citing-turing","text":"To cite Turing, please refer to the following paper. Sample BibTeX entry is given below: @InProceedings{turing17, title = {{T}uring: a language for flexible probabilistic inference}, author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin}, booktitle = {Proceedings of the 21th International Conference on Artificial Intelligence and Statistics}, year = {2018}, series = {Proceedings of Machine Learning Research}, publisher = {PMLR}, }","title":"Citing Turing"},{"location":"#other-probablisticdeep-learning-languages","text":"Stan Infer.NET PyTorch / Pyro TensorFlow / Edward DyNet","title":"Other probablistic/deep learning languages"},{"location":"advanced/","text":"Advanced Usage How to define a customized distribution Turing.jl supports the use of distributions from the Distributions.jl package. By extension it also supports the use of customized distributions, by defining them as sub-types of Distribution type of the Distributions.jl package, as well as corresponding functions. Below shows a workflow of how to define a customized distribution, using the flat prior as a simple example. 1. Define the distribution type The first thing to do is to define a type of the distribution, as a subtype of a corresponding distribution type in the Distributions.jl package. immutable Flat : ContinuousUnivariateDistribution end 2. Define functions for randomness The second thing to do is to define rand() and logpdf() , which will be used to run the model. Distributions.rand(d::Flat) = rand() Distributions.logpdf{T :Real}(d::Flat, x::T) = zero(x) 3. Define helper functions In most cases, it may be required to define helper functions. 3.1 Domain transformation Some helper functions will be used domain transformation. For univariate distributions, the necessary ones are minimum() and maximum() . Distributions.minimum(d::Flat) = -Inf Distributions.maximum(d::Flat) = +Inf Functions for domain transformation which may be required from multi-variate or matrix-variate distributions are size(d) , link(d, x) and invlink(d, x) . Please see src/samplers/support/transform.jl for examples. 3.2 Vectorization support Turing.jl supports a vectorization syntax rv ~ [distribution] , which requires rand() and logpdf() to be called on multiple data points. The functions for Flat are shown below. Distributions.rand(d::Flat, n::Int) = Vector([rand() for _ = 1:n]) Distributions.logpdf{T :Real}(d::Flat, x::Vector{T}) = zero(x) Avoid using @model macro When integrating Turing.jl with other libraries, it's usually necessary to avoid using the @model macro. To achieve this, one needs to understand the @model macro, which basically works as a closure and generates an amended function by Assigning the arguments to corresponding local variables; Adding two keyword arguments vi=VarInfo() and sampler=nothing to the scope Forcing the function to return vi Thus by doing these three steps manually, one could get rid of the @model macro. Taking the gdemo model as an example, the two code sections below (macro and macro-free) have the same effect. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt(s)) x[1] ~ Normal(m, sqrt(s)) x[2] ~ Normal(m, sqrt(s)) return s, m end mf = gdemo([1.5, 2.0]) sample(mf, HMC(1000, 0.1, 5)) # Force Turing.jl to initialize its compiler mf(vi, sampler; x=[1.5, 2.0]) = begin s = Turing.assume(sampler, InverseGamma(2, 3), Turing.VarName(vi, [:c_s, :s], ), vi) m = Turing.assume(sampler, Normal(0,sqrt(s)), Turing.VarName(vi, [:c_m, :m], ), vi) for i = 1:2 Turing.observe(sampler, Normal(m, sqrt(s)), x[i], vi) end vi end mf() = mf(Turing.VarInfo(), nothing) sample(mf, HMC(1000, 0.1, 5)) Note that the use of ~ must be removed due to the fact that in Julia 0.6, ~ is no longer a macro. For this reason, Turing.jl parses ~ within the @model macro to allow for this intuitive notation. Task copying Turing copies Julia tasks to deliver efficient inference algorithms, but it also provides alternative slower implementation as a fallback. Task copying is enabled by default. Task copying requires building a small C program, which should be done automatically on Linux and Mac systems that have GCC and Make installed.","title":"Advanced"},{"location":"advanced/#advanced-usage","text":"","title":"Advanced Usage"},{"location":"advanced/#how-to-define-a-customized-distribution","text":"Turing.jl supports the use of distributions from the Distributions.jl package. By extension it also supports the use of customized distributions, by defining them as sub-types of Distribution type of the Distributions.jl package, as well as corresponding functions. Below shows a workflow of how to define a customized distribution, using the flat prior as a simple example.","title":"How to define a customized distribution"},{"location":"advanced/#1-define-the-distribution-type","text":"The first thing to do is to define a type of the distribution, as a subtype of a corresponding distribution type in the Distributions.jl package. immutable Flat : ContinuousUnivariateDistribution end","title":"1. Define the distribution type"},{"location":"advanced/#2-define-functions-for-randomness","text":"The second thing to do is to define rand() and logpdf() , which will be used to run the model. Distributions.rand(d::Flat) = rand() Distributions.logpdf{T :Real}(d::Flat, x::T) = zero(x)","title":"2. Define functions for randomness"},{"location":"advanced/#3-define-helper-functions","text":"In most cases, it may be required to define helper functions.","title":"3. Define helper functions"},{"location":"advanced/#31-domain-transformation","text":"Some helper functions will be used domain transformation. For univariate distributions, the necessary ones are minimum() and maximum() . Distributions.minimum(d::Flat) = -Inf Distributions.maximum(d::Flat) = +Inf Functions for domain transformation which may be required from multi-variate or matrix-variate distributions are size(d) , link(d, x) and invlink(d, x) . Please see src/samplers/support/transform.jl for examples.","title":"3.1 Domain transformation"},{"location":"advanced/#32-vectorization-support","text":"Turing.jl supports a vectorization syntax rv ~ [distribution] , which requires rand() and logpdf() to be called on multiple data points. The functions for Flat are shown below. Distributions.rand(d::Flat, n::Int) = Vector([rand() for _ = 1:n]) Distributions.logpdf{T :Real}(d::Flat, x::Vector{T}) = zero(x)","title":"3.2 Vectorization support"},{"location":"advanced/#avoid-using-model-macro","text":"When integrating Turing.jl with other libraries, it's usually necessary to avoid using the @model macro. To achieve this, one needs to understand the @model macro, which basically works as a closure and generates an amended function by Assigning the arguments to corresponding local variables; Adding two keyword arguments vi=VarInfo() and sampler=nothing to the scope Forcing the function to return vi Thus by doing these three steps manually, one could get rid of the @model macro. Taking the gdemo model as an example, the two code sections below (macro and macro-free) have the same effect. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt(s)) x[1] ~ Normal(m, sqrt(s)) x[2] ~ Normal(m, sqrt(s)) return s, m end mf = gdemo([1.5, 2.0]) sample(mf, HMC(1000, 0.1, 5)) # Force Turing.jl to initialize its compiler mf(vi, sampler; x=[1.5, 2.0]) = begin s = Turing.assume(sampler, InverseGamma(2, 3), Turing.VarName(vi, [:c_s, :s], ), vi) m = Turing.assume(sampler, Normal(0,sqrt(s)), Turing.VarName(vi, [:c_m, :m], ), vi) for i = 1:2 Turing.observe(sampler, Normal(m, sqrt(s)), x[i], vi) end vi end mf() = mf(Turing.VarInfo(), nothing) sample(mf, HMC(1000, 0.1, 5)) Note that the use of ~ must be removed due to the fact that in Julia 0.6, ~ is no longer a macro. For this reason, Turing.jl parses ~ within the @model macro to allow for this intuitive notation.","title":"Avoid using @model macro"},{"location":"advanced/#task-copying","text":"Turing copies Julia tasks to deliver efficient inference algorithms, but it also provides alternative slower implementation as a fallback. Task copying is enabled by default. Task copying requires building a small C program, which should be done automatically on Linux and Mac systems that have GCC and Make installed.","title":"Task copying"},{"location":"functions/","text":"Function Documentation Function Documentation Modelling Samplers Index Modelling # Turing.@model Macro . @model(name, fbody) Wrapper for models. Usage: @model model() = begin # body end Example: @model gauss() = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) 1.5 ~ Normal(m, sqrt.(s)) 2.0 ~ Normal(m, sqrt.(s)) return(s, m) end source # Turing.@~ Macro . var_name ~ Distribution() Tilde notation ~ can be used to specifiy a variable follows a distributions . If var_name is an un-defined variable or a container (e.g. Vector or Matrix), this variable will be treated as model parameter; otherwise if var_name is defined, this variable will be treated as data. source Samplers # Turing.Sampler Type . Sampler{T} Generic interface for implementing inference algorithms. An implementation of an algorithm should include the following: A type specifying the algorithm and its parameters, derived from InferenceAlgorithm A method of sample function that produces results of inference, which is where actual inference happens. Turing translates models to chunks that call the modelling functions at specified points. The dispatch is based on the value of a sampler variable. To include a new inference algorithm implements the requirements mentioned above in a separate file, then include that file at the end of this one. source # Turing.Gibbs Type . Gibbs(n_iters, alg_1, alg_2) Compositional MCMC interface. Usage: alg = Gibbs(1000, HMC(1, 0.2, 3, :v1), PG(20, 1, :v2)) source # Turing.HMC Type . HMC(n_iters::Int, epsilon::Float64, tau::Int) Hamiltonian Monte Carlo sampler. Usage: HMC(1000, 0.05, 10) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), HMC(1000, 0.05, 10)) source # Turing.HMCDA Type . HMCDA(n_iters::Int, n_adapt::Int, delta::Float64, lambda::Float64) Hamiltonian Monte Carlo sampler wiht Dual Averaging algorithm. Usage: HMCDA(1000, 200, 0.65, 0.3) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), HMCDA(1000, 200, 0.65, 0.3)) source # Turing.IPMCMC Type . IPMCMC(n_particles::Int, n_iters::Int, n_nodes::Int, n_csmc_nodes::Int) Particle Gibbs sampler. Usage: IPMCMC(100, 100, 4, 2) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt(s)) x[1] ~ Normal(m, sqrt(s)) x[2] ~ Normal(m, sqrt(s)) return s, m end sample(gdemo([1.5, 2]), IPMCMC(100, 100, 4, 2)) source # Turing.IS Type . IS(n_particles::Int) Importance sampling algorithm object. n_particles is the number of particles to use Usage: IS(1000) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), IS(1000)) source # Turing.MH Type . MH(n_iters::Int) Metropolis-Hasting sampler. Usage: MH(100, (:m, (x) - Normal(x, 0.1))) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt(s)) x[1] ~ Normal(m, sqrt(s)) x[2] ~ Normal(m, sqrt(s)) return s, m end sample(gdemo([1.5, 2]), MH(1000, (:m, (x) - Normal(x, 0.1)), :s))) source # Turing.NUTS Type . NUTS(n_iters::Int, n_adapt::Int, delta::Float64) No-U-Turn Sampler (NUTS) sampler. Usage: NUTS(1000, 200, 0.6j_max) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.j_max, 2]), NUTS(1000, 200, 0.6j_max)) source # Turing.PG Type . PG(n_particles::Int, n_iters::Int) Particle Gibbs sampler. Usage: PG(100, 100) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), PG(100, 100)) source # Turing.PMMH Type . PMMH(n_iters::Int, smc_alg:::SMC, parameters_algs::Tuple{MH}) Particle independant Metropolis\u2013Hastings and Particle marginal Metropolis\u2013Hastings samplers. Usage: alg = PMMH(100, SMC(20, :v1), MH(1,:v2)) alg = PMMH(100, SMC(20, :v1), MH(1,(:v2, (x) - Normal(x, 1)))) source # Turing.SGHMC Type . SGHMC(n_iters::Int, learning_rate::Float64, momentum_decay::Float64) Stochastic Gradient Hamiltonian Monte Carlo sampler. Usage: SGHMC(1000, 0.01, 0.1) Example: @model example begin ... end sample(example, SGHMC(1000, 0.01, 0.1)) source # Turing.SGLD Type . SGLD(n_iters::Int, step_size::Float64) Stochastic Gradient Langevin Dynamics sampler. Usage: SGLD(1000, 0.5) Example: @model example begin ... end sample(example, SGLD(1000, 0.5)) source # Turing.SMC Type . SMC(n_particles::Int) Sequential Monte Carlo sampler. Usage: SMC(1000) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), SMC(1000)) source Index Turing.Gibbs Turing.HMC Turing.HMCDA Turing.IPMCMC Turing.IS Turing.MH Turing.NUTS Turing.PG Turing.PMMH Turing.SGHMC Turing.SGLD Turing.SMC Turing.Sampler Turing.@model Turing.@~","title":"Functions"},{"location":"functions/#function-documentation","text":"Function Documentation Modelling Samplers Index","title":"Function Documentation"},{"location":"functions/#modelling","text":"# Turing.@model Macro . @model(name, fbody) Wrapper for models. Usage: @model model() = begin # body end Example: @model gauss() = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) 1.5 ~ Normal(m, sqrt.(s)) 2.0 ~ Normal(m, sqrt.(s)) return(s, m) end source # Turing.@~ Macro . var_name ~ Distribution() Tilde notation ~ can be used to specifiy a variable follows a distributions . If var_name is an un-defined variable or a container (e.g. Vector or Matrix), this variable will be treated as model parameter; otherwise if var_name is defined, this variable will be treated as data. source","title":"Modelling"},{"location":"functions/#samplers","text":"# Turing.Sampler Type . Sampler{T} Generic interface for implementing inference algorithms. An implementation of an algorithm should include the following: A type specifying the algorithm and its parameters, derived from InferenceAlgorithm A method of sample function that produces results of inference, which is where actual inference happens. Turing translates models to chunks that call the modelling functions at specified points. The dispatch is based on the value of a sampler variable. To include a new inference algorithm implements the requirements mentioned above in a separate file, then include that file at the end of this one. source # Turing.Gibbs Type . Gibbs(n_iters, alg_1, alg_2) Compositional MCMC interface. Usage: alg = Gibbs(1000, HMC(1, 0.2, 3, :v1), PG(20, 1, :v2)) source # Turing.HMC Type . HMC(n_iters::Int, epsilon::Float64, tau::Int) Hamiltonian Monte Carlo sampler. Usage: HMC(1000, 0.05, 10) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), HMC(1000, 0.05, 10)) source # Turing.HMCDA Type . HMCDA(n_iters::Int, n_adapt::Int, delta::Float64, lambda::Float64) Hamiltonian Monte Carlo sampler wiht Dual Averaging algorithm. Usage: HMCDA(1000, 200, 0.65, 0.3) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), HMCDA(1000, 200, 0.65, 0.3)) source # Turing.IPMCMC Type . IPMCMC(n_particles::Int, n_iters::Int, n_nodes::Int, n_csmc_nodes::Int) Particle Gibbs sampler. Usage: IPMCMC(100, 100, 4, 2) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt(s)) x[1] ~ Normal(m, sqrt(s)) x[2] ~ Normal(m, sqrt(s)) return s, m end sample(gdemo([1.5, 2]), IPMCMC(100, 100, 4, 2)) source # Turing.IS Type . IS(n_particles::Int) Importance sampling algorithm object. n_particles is the number of particles to use Usage: IS(1000) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), IS(1000)) source # Turing.MH Type . MH(n_iters::Int) Metropolis-Hasting sampler. Usage: MH(100, (:m, (x) - Normal(x, 0.1))) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt(s)) x[1] ~ Normal(m, sqrt(s)) x[2] ~ Normal(m, sqrt(s)) return s, m end sample(gdemo([1.5, 2]), MH(1000, (:m, (x) - Normal(x, 0.1)), :s))) source # Turing.NUTS Type . NUTS(n_iters::Int, n_adapt::Int, delta::Float64) No-U-Turn Sampler (NUTS) sampler. Usage: NUTS(1000, 200, 0.6j_max) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.j_max, 2]), NUTS(1000, 200, 0.6j_max)) source # Turing.PG Type . PG(n_particles::Int, n_iters::Int) Particle Gibbs sampler. Usage: PG(100, 100) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), PG(100, 100)) source # Turing.PMMH Type . PMMH(n_iters::Int, smc_alg:::SMC, parameters_algs::Tuple{MH}) Particle independant Metropolis\u2013Hastings and Particle marginal Metropolis\u2013Hastings samplers. Usage: alg = PMMH(100, SMC(20, :v1), MH(1,:v2)) alg = PMMH(100, SMC(20, :v1), MH(1,(:v2, (x) - Normal(x, 1)))) source # Turing.SGHMC Type . SGHMC(n_iters::Int, learning_rate::Float64, momentum_decay::Float64) Stochastic Gradient Hamiltonian Monte Carlo sampler. Usage: SGHMC(1000, 0.01, 0.1) Example: @model example begin ... end sample(example, SGHMC(1000, 0.01, 0.1)) source # Turing.SGLD Type . SGLD(n_iters::Int, step_size::Float64) Stochastic Gradient Langevin Dynamics sampler. Usage: SGLD(1000, 0.5) Example: @model example begin ... end sample(example, SGLD(1000, 0.5)) source # Turing.SMC Type . SMC(n_particles::Int) Sequential Monte Carlo sampler. Usage: SMC(1000) Example: # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt.(s)) x[1] ~ Normal(m, sqrt.(s)) x[2] ~ Normal(m, sqrt.(s)) return s, m end sample(gdemo([1.5, 2]), SMC(1000)) source","title":"Samplers"},{"location":"functions/#index","text":"Turing.Gibbs Turing.HMC Turing.HMCDA Turing.IPMCMC Turing.IS Turing.MH Turing.NUTS Turing.PG Turing.PMMH Turing.SGHMC Turing.SGLD Turing.SMC Turing.Sampler Turing.@model Turing.@~","title":"Index"},{"location":"get-started/","text":"Getting Started Installation To use Turing, you need install Julia first and then install Turing. Install Julia You will need Julia 0.6, which you can get from the official Julia website . It provides three options for users A command line version Julia/downloads ( recommended ) A community maintained IDE Juno JuliaBox.com - a Jupyter notebook in the browser For command line version, we recommend that you install a version downloaded from Julia's official website , as Turing may not work correctly with Julia provided by other sources (e.g. Turing does not work with Julia installed via apt-get due to missing header files). Juno also needs the command line version installed. This IDE is recommended for heavy users who require features like debugging, quick documentation check, etc. JuliaBox provides a pre-installed Jupyter notebook for Julia. You can take a shot at Turing without installing Julia on your machine in few seconds. Install Turing.jl Turing is an officially registered Julia package, so the following should install a stable version of Turing: Pkg.add( Turing ) # Switch to master branch; recommended Pkg.checkout( Turing , master ) Pkg.build( Turing ) Pkg.test( Turing ) [ Recommended ] If you want to use the latest version of Turing with some experimental features, you can try the following instead: Pkg.update() Pkg.clone( Turing ) Pkg.build( Turing ) Pkg.test( Turing ) If all tests pass, you're ready to start using Turing. Basics Introduction A probabilistic program is Julia code wrapped in a @model macro. It can use arbitrary Julia code, but to ensure correctness of inference it should not have external effects or modify global state. Stack-allocated variables are safe, but mutable heap-allocated objects may lead to subtle bugs when using task copying. To help avoid those we provide a Turing-safe datatype TArray that can be used to create mutable arrays in Turing programs. For probabilistic effects, Turing programs should use the ~ notation: x ~ distr where x is a symbol and distr is a distribution. If x is undefined in the model function, inside the probabilistic program, this puts a random variable named x , distributed according to distr , in the current scope. distr can be a value of any type that implements rand(distr) , which samples a value from the distribution distr . If x is defined, this is used for conditioning in a style similar to Anglican (another PPL). Here x should be a value that is observed to have been drawn from the distribution distr . The likelihood is computed using logpdf(distr,y) and should always be positive to ensure correctness of inference algorithms. The observe statements should be arranged so that every possible run traverses all of them in exactly the same order. This is equivalent to demanding that they are not placed inside stochastic control flow. Available inference methods include Importance Sampling (IS), Sequential Monte Carlo (SMC), Particle Gibbs (PG), Hamiltonian Monte Carlo (HMC), Hamiltonian Monte Carlo with Dual Averaging (HMCDA) and The No-U-Turn Sampler (NUTS). Simple Gaussian demo Below is a simple Gaussian demo illustrate the basic usage of Turing.jl # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt(s)) x[1] ~ Normal(m, sqrt(s)) x[2] ~ Normal(m, sqrt(s)) return s, m end Note: for the interests of sanity check, some analytical results on the expectation of samples of this model are E[s] = 49/24 and E[m] = 7/6. Inference methods are functions which take the probabilistic program as one of the arguments. # Run sampler, collect results c1 = sample(gdemo([1.5, 2]), SMC(1000)) c2 = sample(gdemo([1.5, 2]), PG(10,1000)) c3 = sample(gdemo([1.5, 2]), HMC(1000, 0.1, 5)) c4 = sample(gdemo([1.5, 2]), Gibbs(1000, PG(10, 2, :m), HMC(2, 0.1, 5, :s))) c5 = sample(gdemo([1.5, 2]), HMCDA(1000, 0.15, 0.65)) c6 = sample(gdemo([1.5, 2]), NUTS(1000, 0.65)) # Summarise results Mamba.describe(c3) # Plot results p = Mamba.plot(c3) Mamba.draw(p, fmt=:pdf, filename= gdemo-plot.pdf ) The arguments for each sampler are SMC: number of particles PG: number of particles, number of iterations HMC: number of samples, leapfrog step size, leapfrog step numbers Gibbs: number of samples, component sampler 1, component sampler 2, ... HMCDA: number of samples, total leapfrog length, target accept ratio NUTS: number of samples, target accept ratio For detailed information please check Turing.jl's APIs . Modelling syntax explained Models are wrapped by @model with a normal function definition syntax, i.e. @model model_name(arg_1, arg_2) = begin ... end This syntax defines a model which can take data as input to generate a posterior evaluator. The data can be provided either using the same function signature defined, or by using a dictionary containing each argument and its value as pairs, i.e. model_func = model_name(1, 2) model_func = model_name(Dict(:arg_1= 1, :arg_2= 2) This posterior evaluator can then be called by a sampler to run inference, i.e. chn = sample(model_func, HMC(..)) # do inference by sampling using HMC The return chain contains samples of the variables in the model, one can use them do inference, e.g. var_1 = mean(chn[:var_1]) # taking the mean of a variable named var_1 Note that the key should be a symbol. For this reason, in case of fetching x[1] one need to do chn[Symbol(:x[1]) . Turing.jl provides a macro to work around this expression chn[sym\"x[1]\"] . Beyond basics Composition sampling using Gibbs Turing.jl provides a Gibbs interface to combine different samplers. For example, one can combine a HMC sampler with a PG sampler to run inference for different parameters in a single model as below. @model simple_choice(xs) = begin p ~ Beta(2, 2) z ~ Categorical(p) for x = xs if z == 1 x ~ Normal(0, 1) else x ~ Normal(2, 1) end end end simple_choice_f = simple_choice([1.5, 2.0, 0.3]) chn = sample(simple_choice_f, Gibbs(1000, HMC(1,0.2,3,:p), PG(20,1,:z)) For details of composition sampling in Turing.jl, please check the corresponding paper . Working with Mamba.jl Turing.jl wraps its samples using Mamba.Chain so that all the functions working for Mamba.Chain can be re-used in Turing.jl. Two typical functions are Mamba.describe and Mamba.plot , which can be used as follow for an obtained chain chn . using Mamba: describe, plot describe(chn) # gives statistics of the samples plot(chn) # lots statistics of the samples There are a plenty of functions which are useful in Mamaba.jl, e.g. those for convergence diagnostics at here . Changing default settings Some of Turing.jl's default settings can be changed for better usage. AD chunk size Turing.jl uses ForwardDiff.jl for automatic differentiation, which uses the forward-mode chunk-wise AD. The chunk size can be manually set by setchunksize(new_chunk_size) , or alternatively, use an auto-tuning helper function auto_tune_chunk_size!(mf::Function, rep_num=10) which will do simple profile of using different chunk size and choose the best one. Here mf is the model function, e.g. gdemo([1.5, 2]) and rep_num is the number of repetition for profiling. AD backend Since #428 , Turing.jl supports ReverseDiff.jl as backend. To switch between ForwardDiff.jl and ReverseDiff.jl, one can call function setadbackend(backend_sym) , where backend_sym can be :forward_diff or :reverse_diff . Progress meter Turing.jl uses ProgressMeter.jl to show the progress of sampling, which may lead to slow down of inference or even cause bugs in some IDEs due to I/O. This can be turned on or off by turnprogress(true) and turnprogress(false) , of which the former is set as default.","title":"Get started"},{"location":"get-started/#getting-started","text":"","title":"Getting Started"},{"location":"get-started/#installation","text":"To use Turing, you need install Julia first and then install Turing.","title":"Installation"},{"location":"get-started/#install-julia","text":"You will need Julia 0.6, which you can get from the official Julia website . It provides three options for users A command line version Julia/downloads ( recommended ) A community maintained IDE Juno JuliaBox.com - a Jupyter notebook in the browser For command line version, we recommend that you install a version downloaded from Julia's official website , as Turing may not work correctly with Julia provided by other sources (e.g. Turing does not work with Julia installed via apt-get due to missing header files). Juno also needs the command line version installed. This IDE is recommended for heavy users who require features like debugging, quick documentation check, etc. JuliaBox provides a pre-installed Jupyter notebook for Julia. You can take a shot at Turing without installing Julia on your machine in few seconds.","title":"Install Julia"},{"location":"get-started/#install-turingjl","text":"Turing is an officially registered Julia package, so the following should install a stable version of Turing: Pkg.add( Turing ) # Switch to master branch; recommended Pkg.checkout( Turing , master ) Pkg.build( Turing ) Pkg.test( Turing ) [ Recommended ] If you want to use the latest version of Turing with some experimental features, you can try the following instead: Pkg.update() Pkg.clone( Turing ) Pkg.build( Turing ) Pkg.test( Turing ) If all tests pass, you're ready to start using Turing.","title":"Install Turing.jl"},{"location":"get-started/#basics","text":"","title":"Basics"},{"location":"get-started/#introduction","text":"A probabilistic program is Julia code wrapped in a @model macro. It can use arbitrary Julia code, but to ensure correctness of inference it should not have external effects or modify global state. Stack-allocated variables are safe, but mutable heap-allocated objects may lead to subtle bugs when using task copying. To help avoid those we provide a Turing-safe datatype TArray that can be used to create mutable arrays in Turing programs. For probabilistic effects, Turing programs should use the ~ notation: x ~ distr where x is a symbol and distr is a distribution. If x is undefined in the model function, inside the probabilistic program, this puts a random variable named x , distributed according to distr , in the current scope. distr can be a value of any type that implements rand(distr) , which samples a value from the distribution distr . If x is defined, this is used for conditioning in a style similar to Anglican (another PPL). Here x should be a value that is observed to have been drawn from the distribution distr . The likelihood is computed using logpdf(distr,y) and should always be positive to ensure correctness of inference algorithms. The observe statements should be arranged so that every possible run traverses all of them in exactly the same order. This is equivalent to demanding that they are not placed inside stochastic control flow. Available inference methods include Importance Sampling (IS), Sequential Monte Carlo (SMC), Particle Gibbs (PG), Hamiltonian Monte Carlo (HMC), Hamiltonian Monte Carlo with Dual Averaging (HMCDA) and The No-U-Turn Sampler (NUTS).","title":"Introduction"},{"location":"get-started/#simple-gaussian-demo","text":"Below is a simple Gaussian demo illustrate the basic usage of Turing.jl # Define a simple Normal model with unknown mean and variance. @model gdemo(x) = begin s ~ InverseGamma(2,3) m ~ Normal(0,sqrt(s)) x[1] ~ Normal(m, sqrt(s)) x[2] ~ Normal(m, sqrt(s)) return s, m end Note: for the interests of sanity check, some analytical results on the expectation of samples of this model are E[s] = 49/24 and E[m] = 7/6. Inference methods are functions which take the probabilistic program as one of the arguments. # Run sampler, collect results c1 = sample(gdemo([1.5, 2]), SMC(1000)) c2 = sample(gdemo([1.5, 2]), PG(10,1000)) c3 = sample(gdemo([1.5, 2]), HMC(1000, 0.1, 5)) c4 = sample(gdemo([1.5, 2]), Gibbs(1000, PG(10, 2, :m), HMC(2, 0.1, 5, :s))) c5 = sample(gdemo([1.5, 2]), HMCDA(1000, 0.15, 0.65)) c6 = sample(gdemo([1.5, 2]), NUTS(1000, 0.65)) # Summarise results Mamba.describe(c3) # Plot results p = Mamba.plot(c3) Mamba.draw(p, fmt=:pdf, filename= gdemo-plot.pdf ) The arguments for each sampler are SMC: number of particles PG: number of particles, number of iterations HMC: number of samples, leapfrog step size, leapfrog step numbers Gibbs: number of samples, component sampler 1, component sampler 2, ... HMCDA: number of samples, total leapfrog length, target accept ratio NUTS: number of samples, target accept ratio For detailed information please check Turing.jl's APIs .","title":"Simple Gaussian demo"},{"location":"get-started/#modelling-syntax-explained","text":"Models are wrapped by @model with a normal function definition syntax, i.e. @model model_name(arg_1, arg_2) = begin ... end This syntax defines a model which can take data as input to generate a posterior evaluator. The data can be provided either using the same function signature defined, or by using a dictionary containing each argument and its value as pairs, i.e. model_func = model_name(1, 2) model_func = model_name(Dict(:arg_1= 1, :arg_2= 2) This posterior evaluator can then be called by a sampler to run inference, i.e. chn = sample(model_func, HMC(..)) # do inference by sampling using HMC The return chain contains samples of the variables in the model, one can use them do inference, e.g. var_1 = mean(chn[:var_1]) # taking the mean of a variable named var_1 Note that the key should be a symbol. For this reason, in case of fetching x[1] one need to do chn[Symbol(:x[1]) . Turing.jl provides a macro to work around this expression chn[sym\"x[1]\"] .","title":"Modelling syntax explained"},{"location":"get-started/#beyond-basics","text":"","title":"Beyond basics"},{"location":"get-started/#composition-sampling-using-gibbs","text":"Turing.jl provides a Gibbs interface to combine different samplers. For example, one can combine a HMC sampler with a PG sampler to run inference for different parameters in a single model as below. @model simple_choice(xs) = begin p ~ Beta(2, 2) z ~ Categorical(p) for x = xs if z == 1 x ~ Normal(0, 1) else x ~ Normal(2, 1) end end end simple_choice_f = simple_choice([1.5, 2.0, 0.3]) chn = sample(simple_choice_f, Gibbs(1000, HMC(1,0.2,3,:p), PG(20,1,:z)) For details of composition sampling in Turing.jl, please check the corresponding paper .","title":"Composition sampling using Gibbs"},{"location":"get-started/#working-with-mambajl","text":"Turing.jl wraps its samples using Mamba.Chain so that all the functions working for Mamba.Chain can be re-used in Turing.jl. Two typical functions are Mamba.describe and Mamba.plot , which can be used as follow for an obtained chain chn . using Mamba: describe, plot describe(chn) # gives statistics of the samples plot(chn) # lots statistics of the samples There are a plenty of functions which are useful in Mamaba.jl, e.g. those for convergence diagnostics at here .","title":"Working with Mamba.jl"},{"location":"get-started/#changing-default-settings","text":"Some of Turing.jl's default settings can be changed for better usage.","title":"Changing default settings"},{"location":"get-started/#ad-chunk-size","text":"Turing.jl uses ForwardDiff.jl for automatic differentiation, which uses the forward-mode chunk-wise AD. The chunk size can be manually set by setchunksize(new_chunk_size) , or alternatively, use an auto-tuning helper function auto_tune_chunk_size!(mf::Function, rep_num=10) which will do simple profile of using different chunk size and choose the best one. Here mf is the model function, e.g. gdemo([1.5, 2]) and rep_num is the number of repetition for profiling.","title":"AD chunk size"},{"location":"get-started/#ad-backend","text":"Since #428 , Turing.jl supports ReverseDiff.jl as backend. To switch between ForwardDiff.jl and ReverseDiff.jl, one can call function setadbackend(backend_sym) , where backend_sym can be :forward_diff or :reverse_diff .","title":"AD backend"},{"location":"get-started/#progress-meter","text":"Turing.jl uses ProgressMeter.jl to show the progress of sampling, which may lead to slow down of inference or even cause bugs in some IDEs due to I/O. This can be turned on or off by turnprogress(true) and turnprogress(false) , of which the former is set as default.","title":"Progress meter"},{"location":"contributing/guide/","text":"Contributing Turing is an open source project. If you feel you have some relevant skills and are interested in contributing, then please do get in touch. You can contribute by opening issues on Github or implementing things yourself and making a pull request. We would also appreciate example models written using Turing. Turing has a style guide . It is not strictly necessary to review it, but particularly adventurous contributors may enjoy reviewing it. How to Contribute Getting started Fork this repository Clone your fork on your local machine: git clone https://github.com/your_username/Turing.jl Add a remote corresponding to this repository: git remote add upstream https://github.com/TuringLang/Turing.jl What can I do ? Look at the issues page to find an outstanding issue. For instance, you could implement new features, fix bugs or write example models. Git workflow Make sure that your local master branch is up to date with this repository's one ( for more details ): git fetch upstream git checkout master git rebase upstream/master Create a new branch: git checkout -b branch_name (usually use feature-issue_id or bugfix-issue_id ) Do your stuff: git add ... , git commit -m '...' Push your local branch to your fork of this repository: git push --set-upstream origin branch_name Make a pull request Create a pull request by going to this repository front page and selecting Compare pull request If related to a specific issue, link the pull request link in that issue, and in the pull request also link the issue","title":"Guide"},{"location":"contributing/guide/#contributing","text":"Turing is an open source project. If you feel you have some relevant skills and are interested in contributing, then please do get in touch. You can contribute by opening issues on Github or implementing things yourself and making a pull request. We would also appreciate example models written using Turing. Turing has a style guide . It is not strictly necessary to review it, but particularly adventurous contributors may enjoy reviewing it.","title":"Contributing"},{"location":"contributing/guide/#how-to-contribute","text":"","title":"How to Contribute"},{"location":"contributing/guide/#getting-started","text":"Fork this repository Clone your fork on your local machine: git clone https://github.com/your_username/Turing.jl Add a remote corresponding to this repository: git remote add upstream https://github.com/TuringLang/Turing.jl","title":"Getting started"},{"location":"contributing/guide/#what-can-i-do","text":"Look at the issues page to find an outstanding issue. For instance, you could implement new features, fix bugs or write example models.","title":"What can I do ?"},{"location":"contributing/guide/#git-workflow","text":"Make sure that your local master branch is up to date with this repository's one ( for more details ): git fetch upstream git checkout master git rebase upstream/master Create a new branch: git checkout -b branch_name (usually use feature-issue_id or bugfix-issue_id ) Do your stuff: git add ... , git commit -m '...' Push your local branch to your fork of this repository: git push --set-upstream origin branch_name","title":"Git workflow"},{"location":"contributing/guide/#make-a-pull-request","text":"Create a pull request by going to this repository front page and selecting Compare pull request If related to a specific issue, link the pull request link in that issue, and in the pull request also link the issue","title":"Make a pull request"},{"location":"contributing/style_guide/","text":"Style Guide This style guide is adapted from Invenia 's style guide. We would like to thank them for allowing us to access and use it. Please don't let not having read it stop you from contributing to Turing! No one will be annoyed if you open a PR whose style doesn't follow these conventions; we will just help you correct it before it gets merged. These conventions were originally written at Invenia, taking inspiration from a variety of sources including Python's PEP8 , Julia's Notes for Contributors , and Julia's Style Guide . What follows is a mixture of a verbatim copy of Invenia's original guide and some of our own modifications. A Word on Consistency When adhering to this style it's important to realize that these are guidelines and not rules. This is stated best in the PEP8 : A style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is most important. But most importantly: know when to be inconsistent \u2013 sometimes the style guide just doesn't apply. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask! Synopsis Attempt to follow both the Julia Contribution Guidelines , the Julia Style Guide , and this guide. When convention guidelines conflict this guide takes precedence (known conflicts will be noted in this guide). Use 4 spaces per indentation level, no tabs. Try to adhere to a 92 character line length limit. Use upper camel case convention for modules and types . Use lower case with underscores for method names (note: Julia code likes to use lower case without underscores). Comments are good, try to explain the intentions of the code. Use whitespace to make the code more readable. No whitespace at the end of a line (trailing whitespace). Avoid padding brackets with spaces. ex. Int64(value) preferred over Int64( value ) . Editor Configuration Sublime Text Settings If you are a user of Sublime Text we recommend that you have the following options in your Julia syntax specific settings. To modify these settings first open any Julia file ( *.jl ) in Sublime Text. Then navigate to: Preferences Settings - More Syntax Specific - User { translate_tabs_to_spaces : true, tab_size : 4, trim_trailing_white_space_on_save : true, ensure_newline_at_eof_on_save : true, rulers : [92] } Vim Settings If you are a user of Vim we recommend that you add the following options to your .vimrc file. set tabstop=4 Sets tabstops to a width of four columns. set softtabstop=4 Determines the behaviour of TAB and BACKSPACE keys with expandtab. set shiftwidth=4 Determines the results of , , and ==. au FileType julia setlocal expandtab Replaces tabs with spaces. au FileType julia setlocal colorcolumn=93 Highlights column 93 to help maintain the 92 character line limit. By default, Vim seems to guess that .jl files are written in Lisp. To ensure that Vim recognizes Julia files you can manually have it check for the .jl extension, but a better solution is to install Julia-Vim , which also includes proper syntax highlighting and a few cool other features. Atom Settings Atom defaults preferred line length to 80 characters. We want that at 92 for julia. To change it: Go to Atom - Preferences - Packages . Search for the \"language-julia\" package and open the settings for it. Find preferred line length (under \"Julia Grammar\") and change it to 92. Code Formatting Function Naming Names of functions should describe an action or property irrespective of the type of the argument; the argument's type provides this information instead. For example, buyfood(food) should be buy(food::Food) . Names of functions should usually be limited to one or two lowercase words. Ideally write buyfood not buy_food , but if you are writing a function whose name is hard to read without underscores then please do use them. Method Definitions Only use short-form function definitions when they fit on a single line: # Yes: foo(x::Int64) = abs(x) + 3 # No: foobar(array_data::AbstractArray{T}, item::T) where {T :Int64} = T[ abs(x) * abs(item) + 3 for x in array_data ] # No: foobar( array_data::AbstractArray{T}, item::T, ) where {T :Int64} = T[abs(x) * abs(item) + 3 for x in array_data] # Yes: function foobar(array_data::AbstractArray{T}, item::T) where T :Int64 return T[abs(x) * abs(item) + 3 for x in array_data] end When using long-form functions always use the return keyword : # Yes: function fnc(x::T) where T result = zero(T) result += fna(x) return result end # No: function fnc(x::T) where T result = zero(T) result += fna(x) end # Yes: function Foo(x, y) return new(x, y) end # No: function Foo(x, y) new(x, y) end Functions definitions with parameter lines which exceed 92 characters should separate each parameter by a newline and indent by one-level: # Yes: function foobar( df::DataFrame, id::Symbol, variable::Symbol, value::AbstractString, prefix::AbstractString= , ) # code end # Ok: function foobar(df::DataFrame, id::Symbol, variable::Symbol, value::AbstractString, prefix::AbstractString= ) # code end # No: function foobar(df::DataFrame, id::Symbol, variable::Symbol, value::AbstractString, prefix::AbstractString= ) # code end # No: function foobar( df::DataFrame, id::Symbol, variable::Symbol, value::AbstractString, prefix::AbstractString= , ) # code end Keyword Arguments When calling a function always separate your keyword arguments from your positional arguments with a semicolon. This avoids mistakes in ambiguous cases (such as splatting a Dict ). # Yes: xy = foo(x; y=3) # No: xy = foo(x, y=3) Whitespace Avoid extraneous whitespace in the following situations: Immediately inside parentheses, square brackets or braces. julia Yes: spam(ham[1], [eggs]) No: spam( ham[ 1 ], [ eggs ] ) * Immediately before a comma or semicolon: julia Yes: if x == 4 @show(x, y); x, y = y, x end No: if x == 4 @show(x , y) ; x , y = y , x end * When using ranges unless additional operators are used: julia Yes: ham[1:9], ham[1:3:9], ham[1:3:end] No: ham[1: 9], ham[1 : 3: 9] julia Yes: ham[lower:upper], ham[lower:step:upper] Yes: ham[lower + offset : upper + offset] Yes: ham[(lower + offset):(upper + offset)] No: ham[lower + offset:upper + offset] * More than one space around an assignment (or other) operator to align it with another: ``` Yes: x = 1 y = 2 long_variable = 3 No: x = 1 y = 2 long_variable = 3 `` * Always surround these binary operators with a single space on either side: assignment ( = ), [updating operators](https://docs.julialang.org/en/latest/manual/mathematical-operations/#Updating-operators-1) ( += , -= , etc.), [numeric comparisons operators](https://docs.julialang.org/en/latest/manual/mathematical-operations/#Numeric-Comparisons-1) ( == , , , !=`, etc.). Note that this guideline does not apply when performing assignment in method definitions. ``` Yes: i = i + 1 No: i=i+1 Yes: submitted += 1 No: submitted +=1 Yes: x^2 y No: x^2 y ``` * Assignments using expanded array, tuple, or function notation should have the first open bracket on the same line assignment operator and the closing bracket should match the indentation level of the assignment. Alternatively you can perform assignments on a single line when they are short: ```julia Yes: arr = [ 1, 2, 3, ] arr = [ 1, 2, 3, ] result = Function( arg1, arg2, ) arr = [1, 2, 3] # No: arr = [ 1, 2, 3, ] arr = [ 1, 2, 3, ] arr = [ 1, 2, 3, ] ``` Nested array or tuples that are in expanded notation should have the opening and closing brackets at the same indentation level: ```julia Yes: x = [ [ 1, 2, 3, ], [ \"hello\", \"world\", ], ['a', 'b', 'c'], ] No: y = [ [ 1, 2, 3, ], [ \"hello\", \"world\", ], ] z = [[ 1, 2, 3, ], [ \"hello\", \"world\", ], ] ``` * Always include the trailing comma when working with expanded arrays, tuples or functions notation. This allows future edits to easily move elements around or add additional elements. The trailing comma should be excluded when the notation is only on a single-line: ```julia Yes: arr = [ 1, 2, 3, ] result = Function( arg1, arg2, ) arr = [1, 2, 3] No: arr = [ 1, 2, 3 ] result = Function( arg1, arg2 ) arr = [1, 2, 3,] ``` * Triple-quotes use the indentation of the lowest indented line (excluding the opening triple-quote). This means the closing triple-quote should be aligned to least indented line in the string. Triple-backticks should also follow this style even though the indentation does not matter for them. ```julia Yes: str = \"\"\" hello world! \"\"\" str = \"\"\" hello world! \"\"\" cmd = program --flag value parameter No: str = \"\"\" hello world! \"\"\" ``` Comments Comments should be used to state the intended behaviour of code. This is especially important when the code is doing something clever that may not be obvious upon first inspection. Avoid writing comments that state exactly what the code obviously does. # Yes: x = x + 1 # Compensate for border # No: x = x + 1 # Increment x Comments that contradict the code are much worse than no comments. Always make a priority of keeping the comments up-to-date with code changes! Comments should be complete sentences. If a comment is a phrase or sentence, its first word should be capitalized, unless it is an identifier that begins with a lower case letter (never alter the case of identifiers!). If a comment is short, the period at the end can be omitted. Block comments generally consist of one or more paragraphs built out of complete sentences, and each sentence should end in a period. Comments should be separated by at least two spaces from the expression and have a single space after the # . When referencing Julia in documentation note that \"Julia\" refers to the programming language while \"julia\" (typically in backticks, e.g. julia ) refers to the executable. # A commment code # anothher comment more code TODO Documentation It is recommended that most modules, types and functions should have docstrings . That being said, only exported functions are required to be documented. Avoid documenting methods like == as the built in docstring for the function already covers the details well. Try to document a function and not individual methods where possible as typically all methods will have similar docstrings. If you are adding a method to a function which was defined in Base or another package only add a docstring if the behaviour of your function deviates from the existing docstring. Docstrings are written in Markdown and should be concise. Docstring lines should be wrapped at 92 characters. bar(x[, y]) Compute the Bar index between `x` and `y`. If `y` is missing, compute the Bar index between all pairs of columns of `x`. function bar(x, y) ... When types or methods have lots of parameters it may not be feasible to write a concise docstring. In these cases it is recommended you use the templates below. Note if a section doesn't apply or is overly verbose (for example \"Throws\" if your function doesn't throw an exception) it can be excluded. It is recommended that you have a blank line between the headings and the content when the content is of sufficient length. Try to be consistent within a docstring whether you use this additional whitespace. Note that the additional space is only for reading raw markdown and does not effect the rendered version. Type Template (should be skipped if is redundant with the constructor(s) docstring): MyArray{T,N} My super awesome array wrapper! # Fields - `data::AbstractArray{T,N}`: stores the array being wrapped - `metadata::Dict`: stores metadata about the array struct MyArray{T,N} : AbstractArray{T,N} data::AbstractArray{T,N} metadata::Dict end Function Template (only required for exported functions): mysearch(array::MyArray{T}, val::T; verbose=true) where {T} - Int Searches the `array` for the `val`. For some reason we don't want to use Julia's builtin search :) # Arguments - `array::MyArray{T}`: the array to search - `val::T`: the value to search for # Keywords - `verbose::Bool=true`: print out progress details # Returns - `Int`: the index where `val` is located in the `array` # Throws - `NotFoundError`: I guess we could throw an error if `val` isn't found. function mysearch(array::AbstractArray{T}, val::T) where T ... end If your method contains lots of arguments or keywords you may want to exclude them from the method signature on the first line and instead use args... and/or kwargs... . Manager(args...; kwargs...) - Manager A cluster manager which spawns workers. # Arguments - `min_workers::Integer`: The minimum number of workers to spawn or an exception is thrown - `max_workers::Integer`: The requested number of worker to spawn # Keywords - `definition::AbstractString`: Name of the job definition to use. Defaults to the definition used within the current instance. - `name::AbstractString`: ... - `queue::AbstractString`: ... function Manager(...) ... end Feel free to document multiple methods for a function within the same docstring. Be careful to only do this for functions you have defined. Manager(max_workers; kwargs...) Manager(min_workers:max_workers; kwargs...) Manager(min_workers, max_workers; kwargs...) A cluster manager which spawns workers. # Arguments - `min_workers::Int`: The minimum number of workers to spawn or an exception is thrown - `max_workers::Int`: The number of requested workers to spawn # Keywords - `definition::AbstractString`: Name of the job definition to use. Defaults to the definition used within the current instance. - `name::AbstractString`: ... - `queue::AbstractString`: ... function Manager end If the documentation for bullet-point exceeds 92 characters the line should be wrapped and slightly indented. Avoid aligning the text to the : . ... # Keywords - `definition::AbstractString`: Name of the job definition to use. Defaults to the definition used within the current instance. For additional details on documenting in Julia see the official documentation . Test Formatting Testsets Julia provides test sets which allows developers to group tests into logical groupings. Test sets can be nested and ideally packages should only have a single \"root\" test set. It is recommended that the \"runtests.jl\" file contains the root test set which contains the remainder of the tests: @testset PkgExtreme begin include( arithmetic.jl ) include( utils.jl ) end Comparisons Most tests are written in the form @test x == y . Since the == function doesn't take types into account tests like the following are valid: @test 1.0 == 1 . Avoid adding visual noise into test comparisons: # Yes: @test value == 0 # No: @test value == 0.0","title":"Style guide"},{"location":"contributing/style_guide/#style-guide","text":"This style guide is adapted from Invenia 's style guide. We would like to thank them for allowing us to access and use it. Please don't let not having read it stop you from contributing to Turing! No one will be annoyed if you open a PR whose style doesn't follow these conventions; we will just help you correct it before it gets merged. These conventions were originally written at Invenia, taking inspiration from a variety of sources including Python's PEP8 , Julia's Notes for Contributors , and Julia's Style Guide . What follows is a mixture of a verbatim copy of Invenia's original guide and some of our own modifications.","title":"Style Guide"},{"location":"contributing/style_guide/#a-word-on-consistency","text":"When adhering to this style it's important to realize that these are guidelines and not rules. This is stated best in the PEP8 : A style guide is about consistency. Consistency with this style guide is important. Consistency within a project is more important. Consistency within one module or function is most important. But most importantly: know when to be inconsistent \u2013 sometimes the style guide just doesn't apply. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask!","title":"A Word on Consistency"},{"location":"contributing/style_guide/#synopsis","text":"Attempt to follow both the Julia Contribution Guidelines , the Julia Style Guide , and this guide. When convention guidelines conflict this guide takes precedence (known conflicts will be noted in this guide). Use 4 spaces per indentation level, no tabs. Try to adhere to a 92 character line length limit. Use upper camel case convention for modules and types . Use lower case with underscores for method names (note: Julia code likes to use lower case without underscores). Comments are good, try to explain the intentions of the code. Use whitespace to make the code more readable. No whitespace at the end of a line (trailing whitespace). Avoid padding brackets with spaces. ex. Int64(value) preferred over Int64( value ) .","title":"Synopsis"},{"location":"contributing/style_guide/#editor-configuration","text":"","title":"Editor Configuration"},{"location":"contributing/style_guide/#sublime-text-settings","text":"If you are a user of Sublime Text we recommend that you have the following options in your Julia syntax specific settings. To modify these settings first open any Julia file ( *.jl ) in Sublime Text. Then navigate to: Preferences Settings - More Syntax Specific - User { translate_tabs_to_spaces : true, tab_size : 4, trim_trailing_white_space_on_save : true, ensure_newline_at_eof_on_save : true, rulers : [92] }","title":"Sublime Text Settings"},{"location":"contributing/style_guide/#vim-settings","text":"If you are a user of Vim we recommend that you add the following options to your .vimrc file. set tabstop=4 Sets tabstops to a width of four columns. set softtabstop=4 Determines the behaviour of TAB and BACKSPACE keys with expandtab. set shiftwidth=4 Determines the results of , , and ==. au FileType julia setlocal expandtab Replaces tabs with spaces. au FileType julia setlocal colorcolumn=93 Highlights column 93 to help maintain the 92 character line limit. By default, Vim seems to guess that .jl files are written in Lisp. To ensure that Vim recognizes Julia files you can manually have it check for the .jl extension, but a better solution is to install Julia-Vim , which also includes proper syntax highlighting and a few cool other features.","title":"Vim Settings"},{"location":"contributing/style_guide/#atom-settings","text":"Atom defaults preferred line length to 80 characters. We want that at 92 for julia. To change it: Go to Atom - Preferences - Packages . Search for the \"language-julia\" package and open the settings for it. Find preferred line length (under \"Julia Grammar\") and change it to 92.","title":"Atom Settings"},{"location":"contributing/style_guide/#code-formatting","text":"","title":"Code Formatting"},{"location":"contributing/style_guide/#function-naming","text":"Names of functions should describe an action or property irrespective of the type of the argument; the argument's type provides this information instead. For example, buyfood(food) should be buy(food::Food) . Names of functions should usually be limited to one or two lowercase words. Ideally write buyfood not buy_food , but if you are writing a function whose name is hard to read without underscores then please do use them.","title":"Function Naming"},{"location":"contributing/style_guide/#method-definitions","text":"Only use short-form function definitions when they fit on a single line: # Yes: foo(x::Int64) = abs(x) + 3 # No: foobar(array_data::AbstractArray{T}, item::T) where {T :Int64} = T[ abs(x) * abs(item) + 3 for x in array_data ] # No: foobar( array_data::AbstractArray{T}, item::T, ) where {T :Int64} = T[abs(x) * abs(item) + 3 for x in array_data] # Yes: function foobar(array_data::AbstractArray{T}, item::T) where T :Int64 return T[abs(x) * abs(item) + 3 for x in array_data] end When using long-form functions always use the return keyword : # Yes: function fnc(x::T) where T result = zero(T) result += fna(x) return result end # No: function fnc(x::T) where T result = zero(T) result += fna(x) end # Yes: function Foo(x, y) return new(x, y) end # No: function Foo(x, y) new(x, y) end Functions definitions with parameter lines which exceed 92 characters should separate each parameter by a newline and indent by one-level: # Yes: function foobar( df::DataFrame, id::Symbol, variable::Symbol, value::AbstractString, prefix::AbstractString= , ) # code end # Ok: function foobar(df::DataFrame, id::Symbol, variable::Symbol, value::AbstractString, prefix::AbstractString= ) # code end # No: function foobar(df::DataFrame, id::Symbol, variable::Symbol, value::AbstractString, prefix::AbstractString= ) # code end # No: function foobar( df::DataFrame, id::Symbol, variable::Symbol, value::AbstractString, prefix::AbstractString= , ) # code end","title":"Method Definitions"},{"location":"contributing/style_guide/#keyword-arguments","text":"When calling a function always separate your keyword arguments from your positional arguments with a semicolon. This avoids mistakes in ambiguous cases (such as splatting a Dict ). # Yes: xy = foo(x; y=3) # No: xy = foo(x, y=3)","title":"Keyword Arguments"},{"location":"contributing/style_guide/#whitespace","text":"Avoid extraneous whitespace in the following situations: Immediately inside parentheses, square brackets or braces. julia Yes: spam(ham[1], [eggs]) No: spam( ham[ 1 ], [ eggs ] ) * Immediately before a comma or semicolon: julia Yes: if x == 4 @show(x, y); x, y = y, x end No: if x == 4 @show(x , y) ; x , y = y , x end * When using ranges unless additional operators are used: julia Yes: ham[1:9], ham[1:3:9], ham[1:3:end] No: ham[1: 9], ham[1 : 3: 9] julia Yes: ham[lower:upper], ham[lower:step:upper] Yes: ham[lower + offset : upper + offset] Yes: ham[(lower + offset):(upper + offset)] No: ham[lower + offset:upper + offset] * More than one space around an assignment (or other) operator to align it with another: ```","title":"Whitespace"},{"location":"contributing/style_guide/#yes","text":"x = 1 y = 2 long_variable = 3","title":"Yes:"},{"location":"contributing/style_guide/#no","text":"x = 1 y = 2 long_variable = 3 `` * Always surround these binary operators with a single space on either side: assignment ( = ), [updating operators](https://docs.julialang.org/en/latest/manual/mathematical-operations/#Updating-operators-1) ( += , -= , etc.), [numeric comparisons operators](https://docs.julialang.org/en/latest/manual/mathematical-operations/#Numeric-Comparisons-1) ( == , , , !=`, etc.). Note that this guideline does not apply when performing assignment in method definitions. ``` Yes: i = i + 1 No: i=i+1 Yes: submitted += 1 No: submitted +=1 Yes: x^2 y No: x^2 y ``` * Assignments using expanded array, tuple, or function notation should have the first open bracket on the same line assignment operator and the closing bracket should match the indentation level of the assignment. Alternatively you can perform assignments on a single line when they are short: ```julia","title":"No:"},{"location":"contributing/style_guide/#yes_1","text":"arr = [ 1, 2, 3, ] arr = [ 1, 2, 3, ] result = Function( arg1, arg2, ) arr = [1, 2, 3] # No: arr = [ 1, 2, 3, ] arr = [ 1, 2, 3, ] arr = [ 1, 2, 3, ] ``` Nested array or tuples that are in expanded notation should have the opening and closing brackets at the same indentation level: ```julia","title":"Yes:"},{"location":"contributing/style_guide/#yes_2","text":"x = [ [ 1, 2, 3, ], [ \"hello\", \"world\", ], ['a', 'b', 'c'], ]","title":"Yes:"},{"location":"contributing/style_guide/#no_1","text":"y = [ [ 1, 2, 3, ], [ \"hello\", \"world\", ], ] z = [[ 1, 2, 3, ], [ \"hello\", \"world\", ], ] ``` * Always include the trailing comma when working with expanded arrays, tuples or functions notation. This allows future edits to easily move elements around or add additional elements. The trailing comma should be excluded when the notation is only on a single-line: ```julia","title":"No:"},{"location":"contributing/style_guide/#yes_3","text":"arr = [ 1, 2, 3, ] result = Function( arg1, arg2, ) arr = [1, 2, 3]","title":"Yes:"},{"location":"contributing/style_guide/#no_2","text":"arr = [ 1, 2, 3 ] result = Function( arg1, arg2 ) arr = [1, 2, 3,] ``` * Triple-quotes use the indentation of the lowest indented line (excluding the opening triple-quote). This means the closing triple-quote should be aligned to least indented line in the string. Triple-backticks should also follow this style even though the indentation does not matter for them. ```julia","title":"No:"},{"location":"contributing/style_guide/#yes_4","text":"str = \"\"\" hello world! \"\"\" str = \"\"\" hello world! \"\"\" cmd = program --flag value parameter","title":"Yes:"},{"location":"contributing/style_guide/#no_3","text":"str = \"\"\" hello world! \"\"\" ```","title":"No:"},{"location":"contributing/style_guide/#comments","text":"Comments should be used to state the intended behaviour of code. This is especially important when the code is doing something clever that may not be obvious upon first inspection. Avoid writing comments that state exactly what the code obviously does. # Yes: x = x + 1 # Compensate for border # No: x = x + 1 # Increment x Comments that contradict the code are much worse than no comments. Always make a priority of keeping the comments up-to-date with code changes! Comments should be complete sentences. If a comment is a phrase or sentence, its first word should be capitalized, unless it is an identifier that begins with a lower case letter (never alter the case of identifiers!). If a comment is short, the period at the end can be omitted. Block comments generally consist of one or more paragraphs built out of complete sentences, and each sentence should end in a period. Comments should be separated by at least two spaces from the expression and have a single space after the # . When referencing Julia in documentation note that \"Julia\" refers to the programming language while \"julia\" (typically in backticks, e.g. julia ) refers to the executable. # A commment code # anothher comment more code TODO","title":"Comments"},{"location":"contributing/style_guide/#documentation","text":"It is recommended that most modules, types and functions should have docstrings . That being said, only exported functions are required to be documented. Avoid documenting methods like == as the built in docstring for the function already covers the details well. Try to document a function and not individual methods where possible as typically all methods will have similar docstrings. If you are adding a method to a function which was defined in Base or another package only add a docstring if the behaviour of your function deviates from the existing docstring. Docstrings are written in Markdown and should be concise. Docstring lines should be wrapped at 92 characters. bar(x[, y]) Compute the Bar index between `x` and `y`. If `y` is missing, compute the Bar index between all pairs of columns of `x`. function bar(x, y) ... When types or methods have lots of parameters it may not be feasible to write a concise docstring. In these cases it is recommended you use the templates below. Note if a section doesn't apply or is overly verbose (for example \"Throws\" if your function doesn't throw an exception) it can be excluded. It is recommended that you have a blank line between the headings and the content when the content is of sufficient length. Try to be consistent within a docstring whether you use this additional whitespace. Note that the additional space is only for reading raw markdown and does not effect the rendered version. Type Template (should be skipped if is redundant with the constructor(s) docstring): MyArray{T,N} My super awesome array wrapper! # Fields - `data::AbstractArray{T,N}`: stores the array being wrapped - `metadata::Dict`: stores metadata about the array struct MyArray{T,N} : AbstractArray{T,N} data::AbstractArray{T,N} metadata::Dict end Function Template (only required for exported functions): mysearch(array::MyArray{T}, val::T; verbose=true) where {T} - Int Searches the `array` for the `val`. For some reason we don't want to use Julia's builtin search :) # Arguments - `array::MyArray{T}`: the array to search - `val::T`: the value to search for # Keywords - `verbose::Bool=true`: print out progress details # Returns - `Int`: the index where `val` is located in the `array` # Throws - `NotFoundError`: I guess we could throw an error if `val` isn't found. function mysearch(array::AbstractArray{T}, val::T) where T ... end If your method contains lots of arguments or keywords you may want to exclude them from the method signature on the first line and instead use args... and/or kwargs... . Manager(args...; kwargs...) - Manager A cluster manager which spawns workers. # Arguments - `min_workers::Integer`: The minimum number of workers to spawn or an exception is thrown - `max_workers::Integer`: The requested number of worker to spawn # Keywords - `definition::AbstractString`: Name of the job definition to use. Defaults to the definition used within the current instance. - `name::AbstractString`: ... - `queue::AbstractString`: ... function Manager(...) ... end Feel free to document multiple methods for a function within the same docstring. Be careful to only do this for functions you have defined. Manager(max_workers; kwargs...) Manager(min_workers:max_workers; kwargs...) Manager(min_workers, max_workers; kwargs...) A cluster manager which spawns workers. # Arguments - `min_workers::Int`: The minimum number of workers to spawn or an exception is thrown - `max_workers::Int`: The number of requested workers to spawn # Keywords - `definition::AbstractString`: Name of the job definition to use. Defaults to the definition used within the current instance. - `name::AbstractString`: ... - `queue::AbstractString`: ... function Manager end If the documentation for bullet-point exceeds 92 characters the line should be wrapped and slightly indented. Avoid aligning the text to the : . ... # Keywords - `definition::AbstractString`: Name of the job definition to use. Defaults to the definition used within the current instance. For additional details on documenting in Julia see the official documentation .","title":"Documentation"},{"location":"contributing/style_guide/#test-formatting","text":"","title":"Test Formatting"},{"location":"contributing/style_guide/#testsets","text":"Julia provides test sets which allows developers to group tests into logical groupings. Test sets can be nested and ideally packages should only have a single \"root\" test set. It is recommended that the \"runtests.jl\" file contains the root test set which contains the remainder of the tests: @testset PkgExtreme begin include( arithmetic.jl ) include( utils.jl ) end","title":"Testsets"},{"location":"contributing/style_guide/#comparisons","text":"Most tests are written in the form @test x == y . Since the == function doesn't take types into account tests like the following are valid: @test 1.0 == 1 . Avoid adding visual noise into test comparisons: # Yes: @test value == 0 # No: @test value == 0.0","title":"Comparisons"},{"location":"ex/0_Introduction/","text":"Probabilistic Programming with Turing Introduction This notebook is the first of a series of tutorials on the universal probabilistic programming language Turing. Turing is probabilistic programming system written entirely in Julia. It has an intuitive modelling syntax and supports a wide range of sampling-based inference algorithms. Most importantly, Turing inference is composable: it combines Markov chain sampling operations on subsets of model variables, e.g. using a combination of a Hamiltonian Monte Carlo (HMC) engine and a particle Gibbs (PG) engine. This composable inference engine allows the user to easily switch between black-box style inference methods such as HMC and customized inference methods. Familiarity with Julia is assumed through out this notebook. If you are new to Julia, Learning Julia is a good starting point. For users new to Bayesian machine learning, please consider more thorough introductions to the field, such as Pattern Recognition and Machine Learning . This notebook tries to provide an intuition for Bayesian inference and gives a simple example on how to use Turing. Note that this notebook is not a comprehensive introduction to Bayesian machine learning. Coin Flipping without Turing The following example aims to illustrate the effect of updating our beliefs with every piece of new evidence we observe. In particular, we will assume that we are unsure about the probability of heads in a coin flip. To get an intuitive understanding of what \"updating our beliefs\" is, we will visualize the probability of heads in a coin flip after each observed evidence. First, let's load some of the packages we are going to need to flip a coin ( Random , Distributions ) and show our results ( Plots ). You will note that Turing is not an import here \u2013 we are not going to need it for this example. If you are already familiar with posterior updates, you can proceed to the next step. # using Base modules using Random # load a plotting library using Plots # load the distributions library using Distributions Next, we configure our posterior update model. First, let's set the true probability that any coin flip will turn up heads and set the number of coin flips we will show our model: # set the true probability of heads in a coin p_true = 0.5 # iterate from having seen 0 observations to 100 observations Ns = 0:100; We will use the Bernoulli distribution to flip 100 coins, and collect the results in a variable called data . # draw data from a Bernoulli distribution, i.e. draw heads or tails Random.seed!(12) data = rand(Bernoulli(p_true), last(Ns)) # here's what the first five coin flips look like: data[1:5] After flipping all our coins, we want to set a prior belief about what we think the distribution of coinflips look like. In our case, we are going to choose a common prior distribution called the Beta distribution. We will allow this distribution to change as we let our model see more evidence of coin flips. # our prior belief about the probability of heads in a coin toss prior_belief = Beta(1, 1); With our priors set and our data at hand, we can finally run our simple posterior update model. This is a fairly simple process. We expose one additional coin flip to our model every iteratior, such that the first run only sees the first coin flip, while the last iteration sees all the coin flips. Then, we set the updated_belief variable to an updated version of the original Beta distribution after accounting for the new proportion of heads and tails. # this is required for plotting only x = range(0, stop = 1, length = 100) # make an animation animation = @animate for (i, N) in enumerate(Ns) # count the number of heads and tails heads = sum(data[1:i-1]) tails = N - heads # update our prior belief in closed form (this is possible because we use a conjugate prior) updated_belief = Beta(prior_belief.\u03b1 + heads, prior_belief.\u03b2 + tails) # plotting plot(x, pdf.(Ref(updated_belief), x), size = (500, 250), title = Updated belief after $N observations , xlabel = probability of heads , ylabel = , legend = nothing, xlim = (0,1), fill=0, \u03b1=0.3, w=3) vline!([p_true]) end; The animation above shows that with increasing evidence our belief about the probability of heads in a coin flip slowly adjusts towards the true value. The orange line in the animation represents the true probability of seeing heads on a single coin flip, while the mode of the distribution shows what the model believes the probability of a heads is given the evidence it has seen. Coin Flipping with Turing In the previous example, we used the fact that our prior distribution is a conjugate prior . Note that a closed-form expression (the updated_belief expression) for the posterior is not accessible in general and usually does not exist for more interesting models. We are now going to move away from the closed-form expression above and specify the same model using Turing*. To do so, we will first need to import Turing , MCMCChain , Distributions , and StatPlots . MCMChain is a library built by the Turing team to help summarize Markov Chain Monte Carlo (MCMC) simulations, as well as a variety of utility functions for diagnostics and visualizations. # load Turing and MCMCChain using Turing, MCMCChain # load the distributions library using Distributions # load stats plots for density plots using StatPlots First, we will define the coin-flip model using Turing. @model coinflip(y) = begin # our prior belief about the probability of heads in a coin p ~ Beta(1, 1) # the number of observations N = length(y) for n in 1:N # heads or tails of a coin are drawn from a Bernoulli distribution y[n] ~ Bernoulli(p) end end; After defining the model, we can approximate the posterior distribution by pulling samples from the distribution. In this example, we use a Hamiltonian Monte Carlo sampler to construct these samples. Later tutorials will give more information on the samplers available in Turing and discuss their use for different models. # setting of Hamiltonian Monte Carlo (HMC) sampler iterations = 1000 \u03f5 = 0.05 \u03c4 = 10 # start sampling chain = sample(coinflip(data), HMC(iterations, \u03f5, \u03c4)); After finishing the sampling process, we can visualize the posterior distribution approximated using Turing against the posterior distribution in closed-form. We can extract the chain data from the sampler using the Chains(chain[:p]) function. This contains all the values of p we drew while sampling. # construct summary of the sampling process for the parameter p, i.e. the probability of heads in a coin p_summary = Chains(chain[:p]) Object of type Chains Iterations = 1:1000 Thinning interval = 1 Chains = 1 Samples per chain = 1000 [0.859911; 0.219831; \u2026 ; 0.496273; 0.473286] Now we can build our plot: # compute the posterior distribution in closed-form N = length(data) heads = sum(data) updated_belief = Beta(prior_belief.\u03b1 + heads, prior_belief.\u03b2 + N - heads) # visualize a blue density plot of the approximate posterior distribution using HMC (see Chain 1 in the legend) p = densityplot(p_summary, xlim = (0,1), legend = :best, w = 2, c = :blue) # visualize a green density plot of posterior distribution in closed-form plot!(p, range(0, stop = 1, length = 100), pdf.(Ref(updated_belief), range(0, stop = 1, length = 100)), xlabel = probability of heads , ylabel = , title = , xlim = (0,1), label = Closed-form , fill=0, \u03b1=0.3, w=3, c = :lightgreen) # visualize the true probability of heads in red vline!(p, [p_true], label = True probability , c = :red); As we can see, the Turing model closely approximates the true probability. Hopefully this has provided an introduction to Turing's simpler applications. More advanced usage is demonstrated in later tutorials.","title":"0 Introduction"},{"location":"ex/0_Introduction/#probabilistic-programming-with-turing","text":"","title":"Probabilistic Programming with Turing"},{"location":"ex/0_Introduction/#introduction","text":"This notebook is the first of a series of tutorials on the universal probabilistic programming language Turing. Turing is probabilistic programming system written entirely in Julia. It has an intuitive modelling syntax and supports a wide range of sampling-based inference algorithms. Most importantly, Turing inference is composable: it combines Markov chain sampling operations on subsets of model variables, e.g. using a combination of a Hamiltonian Monte Carlo (HMC) engine and a particle Gibbs (PG) engine. This composable inference engine allows the user to easily switch between black-box style inference methods such as HMC and customized inference methods. Familiarity with Julia is assumed through out this notebook. If you are new to Julia, Learning Julia is a good starting point. For users new to Bayesian machine learning, please consider more thorough introductions to the field, such as Pattern Recognition and Machine Learning . This notebook tries to provide an intuition for Bayesian inference and gives a simple example on how to use Turing. Note that this notebook is not a comprehensive introduction to Bayesian machine learning.","title":"Introduction"},{"location":"ex/0_Introduction/#coin-flipping-without-turing","text":"The following example aims to illustrate the effect of updating our beliefs with every piece of new evidence we observe. In particular, we will assume that we are unsure about the probability of heads in a coin flip. To get an intuitive understanding of what \"updating our beliefs\" is, we will visualize the probability of heads in a coin flip after each observed evidence. First, let's load some of the packages we are going to need to flip a coin ( Random , Distributions ) and show our results ( Plots ). You will note that Turing is not an import here \u2013 we are not going to need it for this example. If you are already familiar with posterior updates, you can proceed to the next step. # using Base modules using Random # load a plotting library using Plots # load the distributions library using Distributions Next, we configure our posterior update model. First, let's set the true probability that any coin flip will turn up heads and set the number of coin flips we will show our model: # set the true probability of heads in a coin p_true = 0.5 # iterate from having seen 0 observations to 100 observations Ns = 0:100; We will use the Bernoulli distribution to flip 100 coins, and collect the results in a variable called data . # draw data from a Bernoulli distribution, i.e. draw heads or tails Random.seed!(12) data = rand(Bernoulli(p_true), last(Ns)) # here's what the first five coin flips look like: data[1:5] After flipping all our coins, we want to set a prior belief about what we think the distribution of coinflips look like. In our case, we are going to choose a common prior distribution called the Beta distribution. We will allow this distribution to change as we let our model see more evidence of coin flips. # our prior belief about the probability of heads in a coin toss prior_belief = Beta(1, 1); With our priors set and our data at hand, we can finally run our simple posterior update model. This is a fairly simple process. We expose one additional coin flip to our model every iteratior, such that the first run only sees the first coin flip, while the last iteration sees all the coin flips. Then, we set the updated_belief variable to an updated version of the original Beta distribution after accounting for the new proportion of heads and tails. # this is required for plotting only x = range(0, stop = 1, length = 100) # make an animation animation = @animate for (i, N) in enumerate(Ns) # count the number of heads and tails heads = sum(data[1:i-1]) tails = N - heads # update our prior belief in closed form (this is possible because we use a conjugate prior) updated_belief = Beta(prior_belief.\u03b1 + heads, prior_belief.\u03b2 + tails) # plotting plot(x, pdf.(Ref(updated_belief), x), size = (500, 250), title = Updated belief after $N observations , xlabel = probability of heads , ylabel = , legend = nothing, xlim = (0,1), fill=0, \u03b1=0.3, w=3) vline!([p_true]) end; The animation above shows that with increasing evidence our belief about the probability of heads in a coin flip slowly adjusts towards the true value. The orange line in the animation represents the true probability of seeing heads on a single coin flip, while the mode of the distribution shows what the model believes the probability of a heads is given the evidence it has seen.","title":"Coin Flipping without Turing"},{"location":"ex/0_Introduction/#coin-flipping-with-turing","text":"In the previous example, we used the fact that our prior distribution is a conjugate prior . Note that a closed-form expression (the updated_belief expression) for the posterior is not accessible in general and usually does not exist for more interesting models. We are now going to move away from the closed-form expression above and specify the same model using Turing*. To do so, we will first need to import Turing , MCMCChain , Distributions , and StatPlots . MCMChain is a library built by the Turing team to help summarize Markov Chain Monte Carlo (MCMC) simulations, as well as a variety of utility functions for diagnostics and visualizations. # load Turing and MCMCChain using Turing, MCMCChain # load the distributions library using Distributions # load stats plots for density plots using StatPlots First, we will define the coin-flip model using Turing. @model coinflip(y) = begin # our prior belief about the probability of heads in a coin p ~ Beta(1, 1) # the number of observations N = length(y) for n in 1:N # heads or tails of a coin are drawn from a Bernoulli distribution y[n] ~ Bernoulli(p) end end; After defining the model, we can approximate the posterior distribution by pulling samples from the distribution. In this example, we use a Hamiltonian Monte Carlo sampler to construct these samples. Later tutorials will give more information on the samplers available in Turing and discuss their use for different models. # setting of Hamiltonian Monte Carlo (HMC) sampler iterations = 1000 \u03f5 = 0.05 \u03c4 = 10 # start sampling chain = sample(coinflip(data), HMC(iterations, \u03f5, \u03c4)); After finishing the sampling process, we can visualize the posterior distribution approximated using Turing against the posterior distribution in closed-form. We can extract the chain data from the sampler using the Chains(chain[:p]) function. This contains all the values of p we drew while sampling. # construct summary of the sampling process for the parameter p, i.e. the probability of heads in a coin p_summary = Chains(chain[:p]) Object of type Chains Iterations = 1:1000 Thinning interval = 1 Chains = 1 Samples per chain = 1000 [0.859911; 0.219831; \u2026 ; 0.496273; 0.473286] Now we can build our plot: # compute the posterior distribution in closed-form N = length(data) heads = sum(data) updated_belief = Beta(prior_belief.\u03b1 + heads, prior_belief.\u03b2 + N - heads) # visualize a blue density plot of the approximate posterior distribution using HMC (see Chain 1 in the legend) p = densityplot(p_summary, xlim = (0,1), legend = :best, w = 2, c = :blue) # visualize a green density plot of posterior distribution in closed-form plot!(p, range(0, stop = 1, length = 100), pdf.(Ref(updated_belief), range(0, stop = 1, length = 100)), xlabel = probability of heads , ylabel = , title = , xlim = (0,1), label = Closed-form , fill=0, \u03b1=0.3, w=3, c = :lightgreen) # visualize the true probability of heads in red vline!(p, [p_true], label = True probability , c = :red); As we can see, the Turing model closely approximates the true probability. Hopefully this has provided an introduction to Turing's simpler applications. More advanced usage is demonstrated in later tutorials.","title":"Coin Flipping with Turing"}]}